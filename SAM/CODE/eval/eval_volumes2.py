from myutils import *
import pdb
import pandas as pd
import argparse


def eval_two_volumes_maxpool(target, root, pool_kernel, device):
    # label = read_tiff_stack(target)
    # pre = read_tiff_stack(root)
    label = read_nifti(target)
    pre = read_nifti(root)
    
    k = pool_kernel
    s = max(1, k - 1)
    kernel = (k, k, k)
    pre[pre < 125] = 0
    pre[pre >= 125] = 1
    label[label > 0] = 1
    pre = pre.astype(np.uint8)
    label = label.astype(np.uint8)

    pre = torch.Tensor(pre).view((1, 1, *pre.shape)).to(device)
    label = torch.Tensor(label).view((1, 1, *label.shape)).to(device)

    pre = torch.nn.functional.max_pool3d(pre, kernel, 1, 0)
    label = torch.nn.functional.max_pool3d(label, kernel, 1, 0)

    dice_score = dice_error(pre, label)

    total_loss_iou = iou(pre, label).cpu()
    total_loss_tiou = t_iou(pre, label).cpu()
    clrecall, clprecision, recall, precision = soft_cldice_f1(pre, label)
    cldice = (2. * clrecall * clprecision) / (clrecall + clprecision)

    print('\n Validation IOU: {}\n T-IOU: {}'
          '\n ClDice: {} \n clprecision: {} \n ClRecall: {} \n Dice-score: {}'
          '\n precision: {} \n Recall: {}'
          .format(total_loss_iou, total_loss_tiou, cldice, clprecision, clrecall, dice_score, precision, recall, ':.8f'))
    return {'iou': total_loss_iou,
            'tiou': total_loss_tiou,
            'cldice': cldice,
            'precision': clprecision,
            'recall': clrecall,
            'score': dice_score}


def eval_two_volume_dirs_maxpool(target, root, data, pool_kernel, device):

    if 'trailmap' in root:
        label = read_tiff_stack(join(target, data))
        pre = read_tiff_stack(join(root, 'seg-volume-' + data.split('-')[2]))
    if 'soma' in root:
        if '2471' in root:
            label = read_nifti(join(target, data))
            pre = read_nifti(join(root, 'volume-' + data.split('-')[1]))
        else:
            label = read_nifti(join(target, data))
            pre = read_nifti(join(root, data))
    else:
        label = read_nifti(join(target, data))
        pre = read_nifti(join(root, 'volume-' + data.split('-')[2]))



    k = pool_kernel
    # s = max(1, k - 1)
    kernel = (k, k, k)
    # stride = (s, s, s)
    pre[pre > 0] = 1
    label[label > 0] = 1
    pre = pre.astype(np.uint8)
    label = label.astype(np.uint8)

    pre = torch.Tensor(pre).view((1, 1, *pre.shape)).to(device)
    label = torch.Tensor(label).view((1, 1, *label.shape)).to(device)

    pre = torch.nn.functional.max_pool3d(pre, kernel, 1, 0)
    label = torch.nn.functional.max_pool3d(label, kernel, 1, 0)

    dice_score = dice_error(pre, label)

    total_loss_iou = iou(pre, label).cpu()
    total_loss_tiou = t_iou(pre, label).cpu()
    clrecall, clprecision, recall, precision = soft_cldice_f1(pre, label)
    cldice = (2. * clrecall * clprecision) / (clrecall + clprecision)
    acc = calculate_accuracy(pre, label)

    # print('\n Sub Validation IOU: {}\n T-IOU: {}'
    #       '\n ClDice: {} \n clprecision: {} \n ClRecall: {} \n Dice-score: {}'
    #       '\n precision: {} \n Recall: {}'
    #       .format(total_loss_iou, total_loss_tiou, cldice, clprecision, clrecall, dice_score, precision, recall, ':.8f'))
    return {'iou': total_loss_iou,
            'tiou': total_loss_tiou,
            'cldice': cldice,
            'clprecision': clprecision,
            'clrecall': clrecall,
            'dice': dice_score,
            'precision': precision,
            'recall': recall,
            'acc': acc}


def avg(num, total):
    return total / num


if __name__ == "__main__":
    # ========== evaluate only one cube of 600*600*450 ========== #
    parser = argparse.ArgumentParser()
    parser.add_argument('--target', type=str, default=None, help='grould-truth label of 600*600*450 cube for evaluation')
    parser.add_argument('--root', type=str, default=None, help='data with nnUNet prediction generated by `nnUNet_predict` command')
    parser.add_argument('--kernel_size', type=int, default=3, help='maxpooling kernel size 5,10,15')

    args = parser.parse_args()
    device = torch.device('cuda:2')
    # loss = eval_two_volumes_maxpool(args.target, args.root, args.kernel_size, device)
    # ========== evaluate cubes from a directory(not used)  ========== #
    # data_root = "/media/root/data4/zyt/validate/155829_nii/labels/"  # folder with ground-truth labels
    # data_target = "/media/root/data4/lpq/validate/155829_nii/test/nnunet11/"  # folder with predictions

    # ========================================== Based on all volumes =================================================
    iou_1, tiou_1, cldice_1, clprecision_1, clrecall_1, dice_score_1, recall_1, precision_1, acc_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0
    count = 0

    for data in sorted(os.listdir(args.target)):
        if "12847" in data or "280506" in data or "12816" in data:
            continue
        if ".nii.gz" in data or ".tif" in data or ".tiff" in data:
            count += 1
            loss = eval_two_volume_dirs_maxpool(args.target, args.root, data, args.kernel_size, device)
            iou_1 += loss['iou']
            tiou_1 += loss['tiou']
            cldice_1 += loss['cldice']
            clprecision_1 += loss['clprecision']
            clrecall_1 += loss['clrecall']
            dice_score_1 += loss['dice']
            recall_1 += loss['recall']
            precision_1 += loss['precision']
            acc_1 += loss['acc']
            # print("acc: {}".format(acc_1))
    iou_t = avg(count, iou_1)
    tiou_t = avg(count, tiou_1)
    cldice_t = avg(count, cldice_1)
    clprecision_t = avg(count, clprecision_1)
    clrecall_t = avg(count, clrecall_1)
    score_t = avg(count, dice_score_1)
    recall_t = avg(count, recall_1)
    precision_t = avg(count, precision_1)
    acc_t = avg(count, acc_1)
    print('\n Validation IOU: {}\n T-IOU: {}'
          '\n ClDice: {} \n ClPrecision: {} \n ClRecall: {} \n Dice-score: {} \n Precision: {} \n Recall: {} \n Acc: {}'
          .format(iou_t, tiou_t, cldice_t, clprecision_t, clrecall_t, score_t, precision_t, recall_t, acc_t, '.8f'))
    # =========================================== Based on Brain Regions ==============================================
    # brain_regions = ['BS', 'CB', 'CTX', 'HPF']
    # brain_regions_dict = {}
    # for brain_region in brain_regions:
    #     region_volume_list = []
    #     for data in sorted(os.listdir(args.target)):
    #         if brain_region in data:
    #             region_volume_list.append(data)
    #     brain_regions_dict[brain_region] = region_volume_list
    #
    # # print(brain_regions_dict)
    #
    # for brain_region in brain_regions:
    #     iou_1, tiou_1, cldice_1, clprecision_1, clrecall_1, dice_score_1, recall_1, precision_1, acc_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0
    #     count = 0
    #     for data in brain_regions_dict[brain_region]:
    #         if ".nii.gz" in data or ".tif" in data or ".tiff" in data:
    #             count += 1
    #             loss = eval_two_volume_dirs_maxpool(args.target, args.root, data, args.kernel_size, device)
    #             iou_1 += loss['iou']
    #             tiou_1 += loss['tiou']
    #             cldice_1 += loss['cldice']
    #             clprecision_1 += loss['clprecision']
    #             clrecall_1 += loss['clrecall']
    #             dice_score_1 += loss['dice']
    #             recall_1 += loss['recall']
    #             precision_1 += loss['precision']
    #             acc_1 += loss['acc']
    #             # print("acc: {}".format(acc_1))
    #
    #     # print(count)
    #     iou_t = avg(count, iou_1)
    #     tiou_t = avg(count, tiou_1)
    #     cldice_t = avg(count, cldice_1)
    #     clprecision_t = avg(count, clprecision_1)
    #     clrecall_t = avg(count, clrecall_1)
    #     score_t = avg(count, dice_score_1)
    #     recall_t = avg(count, recall_1)
    #     precision_t = avg(count, precision_1)
    #     acc_t = avg(count, acc_1)
    #     print('=================== {} ====================='.format(brain_region))
    #     print('Validation IOU: {}\n T-IOU: {}'
    #           '\n ClDice: {} \n ClPrecision: {} \n ClRecall: {} \n Dice-score: {} \n Precision: {} \n Recall: {} \n Acc: {}\n'
    #           .format(iou_t, tiou_t, cldice_t, clprecision_t, clrecall_t, score_t, precision_t, recall_t, acc_t, '.8f'))



    # =========================================== Based on Brain Regions and write into excel ==============================================
    # brain_regions = ['BS', 'CB', 'CTX', 'HPF']
    # brain_regions_dict = {}
    #
    # # Categorize data by brain region
    # for brain_region in brain_regions:
    #     region_volume_list = []
    #     for data in sorted(os.listdir(args.target)):
    #         if brain_region in data:
    #             region_volume_list.append(data)
    #     brain_regions_dict[brain_region] = region_volume_list
    #
    # metrics_list = []
    #
    # # Loop through each brain region and calculate the metrics
    # for brain_region in brain_regions:
    #     iou_1, tiou_1, cldice_1, clprecision_1, clrecall_1, dice_score_1, recall_1, precision_1, acc_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0
    #     count = 0
    #
    #     # Process each data file in the region
    #     for data in brain_regions_dict[brain_region]:
    #         if "12847" in data or "280506" in data or "12816" in data:
    #             continue
    #         if ".nii.gz" in data or ".tif" in data or ".tiff" in data:
    #             count += 1
    #             args.kernel_size = 3
    #             loss = eval_two_volume_dirs_maxpool(args.target, args.root, data, args.kernel_size, device)
    #
    #             # Record individual data metrics
    #             metrics_list.append({
    #                 'brain_region': brain_region,
    #                 'data': data,
    #                 'IOU': round(loss['iou'].item(), 5),
    #                 'T-IOU': round(loss['tiou'].item(), 5),
    #                 'ClDice': round(loss['cldice'].item(), 5),
    #                 'ClPrecision': round(loss['clprecision'].item(), 5),
    #                 'ClRecall': round(loss['clrecall'].item(), 5),
    #                 'Dice-score': round(loss['dice'].item(), 5),
    #                 'Recall': round(loss['recall'].item(), 5),
    #                 'Precision': round(loss['precision'].item(), 5),
    #                 'Accuracy': round(loss['acc'].item(), 5)
    #             })
    #
    #             # Accumulate metrics for averaging
    #             iou_1 += loss['iou'].item()
    #             tiou_1 += loss['tiou'].item()
    #             cldice_1 += loss['cldice'].item()
    #             clprecision_1 += loss['clprecision'].item()
    #             clrecall_1 += loss['clrecall'].item()
    #             dice_score_1 += loss['dice'].item()
    #             recall_1 += loss['recall'].item()
    #             precision_1 += loss['precision'].item()
    #             acc_1 += loss['acc'].item()
    #
    #     # Calculate average metrics for the current brain region
    #     iou_t = round(avg(count, iou_1), 5)
    #     tiou_t = round(avg(count, tiou_1), 5)
    #     cldice_t = round(avg(count, cldice_1), 5)
    #     clprecision_t = round(avg(count, clprecision_1), 5)
    #     clrecall_t = round(avg(count, clrecall_1), 5)
    #     score_t = round(avg(count, dice_score_1), 5)
    #     recall_t = round(avg(count, recall_1), 5)
    #     precision_t = round(avg(count, precision_1), 5)
    #     acc_t = round(avg(count, acc_1), 5)
    #
    #     # Record average metrics for the brain region
    #     metrics_list.append({
    #         'brain_region': brain_region,
    #         'data': 'Average',
    #         'IOU': iou_t,
    #         'T-IOU': tiou_t,
    #         'ClDice': cldice_t,
    #         'ClPrecision': clprecision_t,
    #         'ClRecall': clrecall_t,
    #         'Dice-score': score_t,
    #         'Recall': recall_t,
    #         'Precision': precision_t,
    #         'Accuracy': acc_t
    #     })
    #
    # # Create a DataFrame from the list of metrics
    # df = pd.DataFrame(metrics_list)
    #
    # # Write the DataFrame to an Excel file
    # excel_name = '/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/brain_region_metrics_trailmap_k_'+ str(args.kernel_size) +'.xlsx'
    # df.to_excel(excel_name, index=False)
    #
    # print('Metrics for each data and brain region averages saved to {}'.format(excel_name))



    # =========================================== Based on stages or methods and write into excel ==============================================
    # # methods = ['Trailmap', 'NM', 'OURS']
    # # methods = ['OURS', 'MAE', 'NO_pretrain']
    # # methods = ['OURS', 'no_HOG', 'StarDist', 'Cellpose', 'ClearMap']
    # methods = ['11', '21', '51', '201']
    # metrics_list = []
    #
    # # Loop through each brain region and calculate the metrics
    # for method in methods:
    #     iou_1, tiou_1, cldice_1, clprecision_1, clrecall_1, dice_score_1, recall_1, precision_1, acc_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0
    #     count = 0
    #
    #     ### axon
    #     # if method == 'Trailmap':
    #     #     args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/masks_tiff/"
    #     #     args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/trailmap_pre_08/"
    #     # if method == 'NM':
    #     #     args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/masks/"
    #     #     args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/nn_pre/"
    #     # if method == 'NO_pretrain':
    #     #     args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/masks/"
    #     #     args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/no_pretrain/"
    #     # if method == 'MAE':
    #     #     args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/masks/"
    #     #     args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/mae_pre/"
    #     # elif method == 'OURS':
    #     #     args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/masks/"
    #     #     args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/mae_hog_pre/"
    #
    #     ### soma
    #     if method == 'StarDist':
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/stardist/"
    #     if method == 'Cellpose':
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/cellpose/"
    #     if method == 'ClearMap':
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/clearmap/mask/"
    #     if method == 'MAE':
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24712/"
    #     if method == 'NO_pretrain':
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24712_nopretrain/"
    #     if method == '21':
    #         # args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         # args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24713_hog/"
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/result_3/0.5"
    #     if method == '51':
    #         # args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         # args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24714_hog/"
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/result_3/0.2"
    #     if method == '201':
    #         # args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         # args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24715_hog/"
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/result_3/0.05"
    #     elif method == 'OURS' or method == '11':
    #         # args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels_nii/"
    #         # args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/mae_pre_24712_hog/"
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/labels/"
    #         args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/soma/stardist/"
    #
    #
    #     for data in sorted(os.listdir(args.target)):
    #         if "12847" in data or "280506" in data or "12816" in data:
    #             continue
    #         # if "volume-128" in data or "volume-28" in data:
    #         else:
    #             count += 1
    #             args.kernel_size = 3
    #             loss = eval_two_volume_dirs_maxpool(args.target, args.root, data, args.kernel_size, device)
    #
    #             # Record individual data metrics
    #             metrics_list.append({
    #                 'method': method,
    #                 'data': data,
    #                 'IOU': round(loss['iou'].item(), 5),
    #                 'T-IOU': round(loss['tiou'].item(), 5),
    #                 'ClDice': round(loss['cldice'].item(), 5),
    #                 'ClPrecision': round(loss['clprecision'].item(), 5),
    #                 'ClRecall': round(loss['clrecall'].item(), 5),
    #                 'Dice-score': round(loss['dice'].item(), 5),
    #                 'Recall': round(loss['recall'].item(), 5),
    #                 'Precision': round(loss['precision'].item(), 5),
    #                 'Accuracy': round(loss['acc'].item(), 5)
    #             })
    #
    #             # Accumulate metrics for averaging
    #             iou_1 += loss['iou'].item()
    #             tiou_1 += loss['tiou'].item()
    #             cldice_1 += loss['cldice'].item()
    #             clprecision_1 += loss['clprecision'].item()
    #             clrecall_1 += loss['clrecall'].item()
    #             dice_score_1 += loss['dice'].item()
    #             recall_1 += loss['recall'].item()
    #             precision_1 += loss['precision'].item()
    #             acc_1 += loss['acc'].item()
    #
    #     # Calculate average metrics for the current brain region
    #     iou_t = round(avg(count, iou_1), 5)
    #     tiou_t = round(avg(count, tiou_1), 5)
    #     cldice_t = round(avg(count, cldice_1), 5)
    #     clprecision_t = round(avg(count, clprecision_1), 5)
    #     clrecall_t = round(avg(count, clrecall_1), 5)
    #     score_t = round(avg(count, dice_score_1), 5)
    #     recall_t = round(avg(count, recall_1), 5)
    #     precision_t = round(avg(count, precision_1), 5)
    #     acc_t = round(avg(count, acc_1), 5)
    #
    #     # Record average metrics for the brain region
    #     metrics_list.append({
    #         'method': method,
    #         'data': 'Average',
    #         'IOU': iou_t,
    #         'T-IOU': tiou_t,
    #         'ClDice': cldice_t,
    #         'ClPrecision': clprecision_t,
    #         'ClRecall': clrecall_t,
    #         'Dice-score': score_t,
    #         'Recall': recall_t,
    #         'Precision': precision_t,
    #         'Accuracy': acc_t
    #     })
    #
    # # Create a DataFrame from the list of metrics
    # df = pd.DataFrame(metrics_list)
    #
    # # Write the DataFrame to an Excel file
    # excel_name = '/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/brain_region_metrics_soma_stardist_label_efficiency_k_' + str(
    #     args.kernel_size) + '_new.xlsx'
    # df.to_excel(excel_name, index=False)
    #
    # print('Metrics for each data and brain region averages saved to {}'.format(excel_name))



    # =========================================== Label efficiency and write into excel ==============================================
    # methods = ['NM', 'MAE']
    # label_efficiencies = ['11', '21', '51', '201']
    # metrics_list = []
    #
    # # Loop through each brain region and calculate the metrics
    # for label_efficiency in label_efficiencies:
    #     for method in methods:
    #         iou_1, tiou_1, cldice_1, clprecision_1, clrecall_1, dice_score_1, recall_1, precision_1, acc_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0
    #         count = 0
    #         args.target = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/label_efficiency/P10/masks/"
    #         if method == 'NM':
    #             args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/label_efficiency/P10/nn_pre/" + label_efficiency
    #         elif method == 'MAE':
    #             args.root = "/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/label_efficiency/P10/mae_pre/" + label_efficiency
    #
    #         for data in sorted(os.listdir(args.target)):
    #             if "12847" in data or "280506" in data or "12816" in data:
    #                 continue
    #             # if "volume-128" in data or "volume-28" in data:
    #             else:
    #                 count += 1
    #                 args.kernel_size = 2
    #                 loss = eval_two_volume_dirs_maxpool(args.target, args.root, data, args.kernel_size, device)
    #
    #                 # Record individual data metrics
    #                 metrics_list.append({
    #                     'method': method + '-' + label_efficiency,
    #                     'data': data,
    #                     'IOU': round(loss['iou'].item(), 5),
    #                     'T-IOU': round(loss['tiou'].item(), 5),
    #                     'ClDice': round(loss['cldice'].item(), 5),
    #                     'ClPrecision': round(loss['clprecision'].item(), 5),
    #                     'ClRecall': round(loss['clrecall'].item(), 5),
    #                     'Dice-score': round(loss['dice'].item(), 5),
    #                     'Recall': round(loss['recall'].item(), 5),
    #                     'Precision': round(loss['precision'].item(), 5),
    #                     'Accuracy': round(loss['acc'].item(), 5)
    #                 })
    #
    #                 # Accumulate metrics for averaging
    #                 iou_1 += loss['iou'].item()
    #                 tiou_1 += loss['tiou'].item()
    #                 cldice_1 += loss['cldice'].item()
    #                 clprecision_1 += loss['clprecision'].item()
    #                 clrecall_1 += loss['clrecall'].item()
    #                 dice_score_1 += loss['dice'].item()
    #                 recall_1 += loss['recall'].item()
    #                 precision_1 += loss['precision'].item()
    #                 acc_1 += loss['acc'].item()
    #
    #         # Calculate average metrics for the current brain region
    #         iou_t = round(avg(count, iou_1), 5)
    #         tiou_t = round(avg(count, tiou_1), 5)
    #         cldice_t = round(avg(count, cldice_1), 5)
    #         clprecision_t = round(avg(count, clprecision_1), 5)
    #         clrecall_t = round(avg(count, clrecall_1), 5)
    #         score_t = round(avg(count, dice_score_1), 5)
    #         recall_t = round(avg(count, recall_1), 5)
    #         precision_t = round(avg(count, precision_1), 5)
    #         acc_t = round(avg(count, acc_1), 5)
    #
    #         # Record average metrics for the brain region
    #         metrics_list.append({
    #             'method': method + '_' + label_efficiency,
    #             'data': 'Average',
    #             'IOU': iou_t,
    #             'T-IOU': tiou_t,
    #             'ClDice': cldice_t,
    #             'ClPrecision': clprecision_t,
    #             'ClRecall': clrecall_t,
    #             'Dice-score': score_t,
    #             'Recall': recall_t,
    #             'Precision': precision_t,
    #             'Accuracy': acc_t
    #         })
    #
    # # Create a DataFrame from the list of metrics
    # df = pd.DataFrame(metrics_list)
    #
    # # Write the DataFrame to an Excel file
    # excel_name = '/media/root/18TB_HDD/lpq/TH_validate/brain_region_cubes/collected/brain_region_metrics_label_efficiency_P10_k_' + str(
    #     args.kernel_size) + '.xlsx'
    # df.to_excel(excel_name, index=False)
    #
    # print('Metrics for each data and brain region averages saved to {}'.format(excel_name))


